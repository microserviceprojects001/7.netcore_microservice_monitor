# 配置

```
<source>
  @type http
  port 9880
  bind 0.0.0.0
  tag http.input
  <parse>
    @type json
  </parse>
</source>

<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<filter **>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "my_windows_app"
  </record>
</filter>

<filter **>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field false
  <parse>
    @type json
  </parse>
</filter>

<match **>
  @type elasticsearch
  host es-master
  port 9200
  logstash_format true
  logstash_prefix fluentd
  flush_interval 5s
</match>

<match **>
  @type stdout
</match>
```

📘 第一部分：Fluentd 配置详解

![alt text](<../截图/11. Fluentd配置详解.png>)
现在，我们来详细解读图中的每一个关键组件：

1. 输入源 (<source>)
   这是日志的入口，定义了 Fluentd 从哪里、以何种协议接收数据。

- @type http：开启一个 HTTP 端点，接收 POST 请求。您可以用 curl 或任何程序直接发送 JSON 日志到 http://localhost:9880。<parse> 块指定了如何解析请求体。

- @type forward：这是 Fluentd 的原生、高性能二进制协议。Docker 的 --log-driver=fluentd 使用的正是这个协议，这也是您的日志主要入口。

2. 过滤器链 (<filter>)
   日志进入后，会经过一系列过滤器进行处理，顺序至关重要。

- record_transformer：在每条日志记录中添加固定字段。您看到的 hostname 和 service 就是这里添加的，便于标识日志来源。
- parser (您攻克的核心)：这是实现结构化日志的关键。

key_name log：指定要解析的字段是 log。

@type json：将 log 字段的值（一个 JSON 字符串）解析成键值对。

reserve_data true：解析后，保留原有所有字段（如 container_id）。如果设为 false，则原始 log 字段会被移除，只保留解析后的字段。

- 原始数据接收：你的 .NET 应用通过 Docker 驱动将 JSON 日志发送给 Fluentd。Fluentd 为其添加了 hostname、service 等字段后，这条记录被初步存储到 ES，此时 log 字段是一个完整的 JSON 字符串。

- 解析器工作：你配置的 parser 过滤器 (key_name log, @type json) 会：

定位字段：在每条流经的记录中，找到 key_name 指定的 log 字段。

执行解析：将该字段的值（那个长的 JSON 字符串）用 @type json 解析器进行解析，将其拆解成一个个键值对。

- 数据重塑：解析后，原来嵌套在 log 字符串内的 Timestamp、Level、MessageTemplate 等键值对，会被提升（Promote）为与 log、container_name 等同级的顶级字段。这正是你希望达到的“字段展开”效果

3. 输出目的地 (<match>)
   定义处理后的日志发往何处。日志会流经所有匹配的 match。

- elasticsearch：核心输出。host es-master 利用了 Docker 网络，通过容器名直连。logstash_format true 会自动创建按日滚动的索引（fluentd-年.月.日），这是管理日志的最佳实践。

- stdout：将日志打印到 Fluentd 容器的标准输出，是调试配置的利器。您之前用它发现了标签的真实值。
