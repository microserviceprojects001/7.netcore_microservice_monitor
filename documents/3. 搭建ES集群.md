# docker-compose.yml

```
version: '3.8'

services:
  # 专用主节点 - 负责集群管理，不存储数据分片
  es-master:
    image: registry.cn-hangzhou.aliyuncs.com/abelcontainer/elasticsearch:8.11.0
    container_name: es-master
    environment:
      - node.name=es-master
      - cluster.name=my-es-cluster  # 集群名称，三个节点必须相同
      - discovery.seed_hosts=es-master,es-data1,es-data2  # 节点发现列表
      - cluster.initial_master_nodes=es-master,es-data1,es-data2  # 初始候选主节点
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms400m -Xmx400m"  # JVM堆内存，根据您机器内存调整
      - node.roles=master  # 此容器仅作为主节点角色
      - xpack.security.enabled=false  # 开发环境禁用安全认证（生产环境必须开启！）
    ulimits:
      memlock:
        soft: -1
        hard: -1
    mem_limit: 800m  # Docker容器总内存限制
    ports:
      - "9200:9200"  # 宿主机端口9200映射到此容器的9200 (REST API)
      - "9300:9300"  # 宿主机端口9300映射到此容器的9300 (集群内部通信)
    networks:
      - elastic

  # 数据节点1 - 负责存储数据和执行查询，可持有主分片和副本分片
  es-data1:
    image: registry.cn-hangzhou.aliyuncs.com/abelcontainer/elasticsearch:8.11.0
    container_name: es-data1
    environment:
      - node.name=es-data1
      - cluster.name=my-es-cluster
      - discovery.seed_hosts=es-master,es-data1,es-data2
      - cluster.initial_master_nodes=es-master,es-data1,es-data2
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms400m -Xmx400m"
      - node.roles=data  # 此容器仅作为数据节点角色
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    mem_limit: 800m
    volumes:
      - es-data1:/usr/share/elasticsearch/data  # 使用命名卷持久化数据，避免Windows路径问题
    ports:
      - "9201:9200"  # 宿主机端口9201映射到此容器的9200
      - "9301:9300"  # 宿主机端口9301映射到此容器的9300
    networks:
      - elastic

  # 数据节点2 - 另一个数据节点，用于构成副本和高可用
  es-data2:
    image: registry.cn-hangzhou.aliyuncs.com/abelcontainer/elasticsearch:8.11.0
    container_name: es-data2
    environment:
      - node.name=es-data2
      - cluster.name=my-es-cluster
      - discovery.seed_hosts=es-master,es-data1,es-data2
      - cluster.initial_master_nodes=es-master,es-data1,es-data2
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms400m -Xmx400m"
      - node.roles=data
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    mem_limit: 800m
    volumes:
      - es-data2:/usr/share/elasticsearch/data
    ports:
      - "9202:9200"  # 宿主机端口9202映射到此容器的9200
      - "9302:9300"  # 宿主机端口9302映射到此容器的9300
    networks:
      - elastic

# 定义数据卷，数据将持久化保存在这里
volumes:
  es-data1:
  es-data2:

# 定义容器使用的网络，便于彼此发现和通信
networks:
  elastic:
    driver: bridge

```

docker-compose up -d

右键 yml 文件可以直接启动 docker-compose.yml
需要稍微等一下，容器完全启动完成后,可访问:

http://localhost:9200/

# 集群内容器的通信过程

在 Docker 的桥接网络 (elastic) 中，容器间的直接通信，是使用它们的“容器 IP”和“应用监听端口”，而不会绕道“宿主机映射端口”

## 真实流程详解

假设三个容器在同一个用户定义的桥接网络 elastic 中，Docker 会为它们分配容器 IP（如 172.20.0.2, .3, .4），并内置一个 DNS，使得容器名（如 es-master）能直接解析到对应的容器 IP。

1. 容器启动：es-data1 容器启动，其内部的 Elasticsearch 进程监听在 :9300。

2. 发起连接：es-data1 的进程读取 discovery.seed_hosts 配置，获得第一个主机名 es-master。它通过 Docker 内置的 DNS 解析 es-master，得到其容器 IP（例如 172.20.0.2）。

3. 建立 TCP 连接：es-data1 的进程向目标地址 172.20.0.2:9300 （即 es-master 容器的 :9300）发起一个 TCP 连接请求。

4. 握手与通信：TCP 连接在容器网络内部直接建立。随后，所有集群发现、元数据交换等消息都通过这个 容器 IP:容器端口 的通道进行。

5. 关于宿主机端口 (9300, 9301, 9302)：这些端口映射（-p 9301:9300）的主要作用是允许宿主机本身或其他外部网络（非 elastic 网络）的服务来访问这些容器。对于同属于 elastic 网络的容器间通信，它们不会被用到。

   ![alt text](<../截图/1. 集群通信流程.png>)

## 那么，宿主机端口映射（-p 9301:9300）是干什么用的？

它的作用是 打通 Docker 虚拟网络和宿主机网络之间的“墙”，提供一个让宿主机本身或其他外部网络能够访问到容器内服务的通道。

用途：例如，您在本机用 curl 测试，或者另一个运行在宿主机上、但不在 elastic 网络里的应用需要连接 ES。

类比：这就像公司的内网（elastic 网络）。员工间打电话用短号（容器名），速度快且方便（内部通信）。而总机（宿主机端口映射）负责转接外线电话进来，或者让员工能打外线出去。

# 结论总结

集群握手与内部通信：100% 通过 Docker 虚拟网络，使用容器名(DNS) -> 容器 IP -> 直接路由完成。与 -p 映射的宿主机端口无关。

宿主机端口映射：是外部访问的入口，用于从“物理世界”访问“虚拟网络”内的服务。

# 遇到问题

浏览器访问 http://localhost:9200/\_cluster/health?pretty
报错：

```
{
  "error" : {
    "root_cause" : [
      {
        "type" : "master_not_discovered_exception",
        "reason" : null
      }
    ],
    "type" : "master_not_discovered_exception",
    "reason" : null
  },
  "status" : 503
}
```

：“在初始引导时，请等待 es-master、es-data1、es-data2 这三个节点都作为‘候选主节点’出现，然后我们再进行选举。”
node.roles=data
这条配置却告诉 es-data1 和 es-data2：“你们俩只是数据节点，不要参与主节点选举。”

这就造成了矛盾：集群在等待 es-data1 和 es-data2 以“候选主节点”的身份加入，但它们却以“纯数据节点”的身份加入。导致集群永远等不到足够的“候选主节点”来形成法定人数（quorum），选举失败。

# 方案

方案一（推荐，更符合您的初始意图）：将所有节点都设为“候选主节点”
修改 docker-compose.yml，为 es-data1 和 es-data2 的 node.roles 添加 master 角色。这样它们就有资格参与选举，满足 cluster.initial_master_nodes 的期待。

方案二（仅保留一个主节点）：修改初始主节点列表
如果希望 es-data1 和 es-data2 保持为纯数据节点，则 cluster.initial_master_nodes 应只包含真正的候选主节点，即 es-master。

考虑到您只有三个节点，为了保证高可用性（即使一个节点宕机，集群管理功能依然正常），强烈推荐方案一。在生产环境中，候选主节点通常也是奇数个（3，5，7）并分散部署

此时 浏览器请求
http://localhost:9200/\_cluster/health?pretty

```
{
  "cluster_name" : "my-es-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
```
